# Background

## What is cookie clicker?

Cookie clicker is an online browser-based game my daughter introduced me to. The objective of the game is to get as many cookies as possible. You can then use cookies as a currency to buy buildings and upgrades that will give you more cookies. In other words you spend cookies to make cookies and it's really fun.

## Why do you want to hack it?

For fun.... and to try out Gemini-CLI.

## What are you hacking?

The game has an `export` function, where you can save your current progress to a long string of characers. Similarly it has an `import` function that enables you to resume your game from anywhere by simply loading in this string. 

Hence the string must contain all details about your current state. Which means we can modify this state -- giving us plenty more cookies that we earned legitimately.

All this diabolical-ness is getting too exciting. But how do we hack it?

# Approach

The approach is simple. I went to [cookieclicker.com](https://cookieclicker.com) and then poked around using Chrome Developer Tools. There I found a `main.js` file that was conveniently the entire game in a single ~16,000 line file.

`main.js` wasn't minified, and all human readable together with comments.

I then bootstrapped a simple directory on my local mac with:

* A JJ repository to track changes
* Initialized a uv environment for my python code
* Add `black` and `pytest` to the my environment
* Drafted a small `GEMINI.md` file to instruct the agent
* Created two folders, `/src` and `/docs`. For code and docs.
* Copied `main.js` into the root directory.

Then I started up Gemini-CLI and starting working.

## Did it work?

TL:DR Yes it did.

## Did you manage to get all the cookies?

Yes, the final python code allowed me to modify any field, to get as many cookies,buildings, and upgrades/achievements I wanted. The repo also had extensive documentation generated by the agent.

## Did the agent do all the work?

No. I jumped in occassionally to refactor/rewrite some of the code.

## Did the agent solve it in one prompt?

No. And I don't believe any amount of prompting would have succeeded in a single prompt. That said, my prompt writing skills aren't great either.

#  Lessons

## Research before doing

Because this took real code from the real-world. I had to first analyze the 16,000lines of Javascript. I asked Gemini to write documentation, starting at a high level and then drilling deeper into areas I knew I wanted to cover.

I read every bit of documentation, so that my knowledge of the running program was also growing over time. This allowed me to guide the agent better.

Breaking down the documentation also worked well, and the agent was able to ingest all 16,000 lines was also quite amazing.

## Plan before doing

Asking the agent for a plan before it begins executing also worked well. Twice in the conversation I had to stop the agent from executing, and instead asked it for a plan of how to resolve the issue.

## Dual screen terminals for the win

I asked the agent to output documentation to `.md` files in markdown. On a split screen terminal I have Gemini-CLI on one terminal, and another for other things. I used `glow` to view `.md` files and `nvim` to modify if needed. But when I needed to really debug, I dropped into VSCode, I still prefer debugging in a visual editor.

## JJ

I'm also learning `jj` at the same time. JJ works a little better than git as it forced me to think about stuff **before** the commit rather than after. It also has a nicer log. The best part is that I would create a new commit in JJ, and then save the corresponding chat in gemini-cli via `/chat save commit-name` to allow to restore everything back in case the agent went hay-wire.

Fortunately, the agent didn't go hay-wire and I never had to go back revisions. But knowing that I would be one `jj abandon` away from restoring everything was nice.

## Small is better than Big

Everyone knows at least one Agile practioner who took agile a bit too seriously than necessary. But, at the core of Agile lies an important truth -- small changes are better than big changes.

If you prefer small PRs -- then treat each agent execution as a small PR. Do not ask the agent to perform large unsupervised task. Instead ask the agent to go through small steps, and then check in on each step. Only after a few successive small steps do you try to increase the steps.

I'm definitely in the small-changes camp. I know some folks prefer spending time writing up the detail specification for the AI to write -- but if you wouldn't like to merge a 10,000 line PR, why ask the agent to generate 10,000 lines of code in one go. I personally still don't trust the agents to write large changes from the get-go.

## Guide the Agent

The biggest difference between what I did, and what most folks do with agents, is I took a real-world example and tried to dissect it. I've learnt over the years that there is one (maybe two) ways of doing things right -- but a million ways to do things wrong.

Everyone gets things wrong in their own unique way. Which means when an AI agent (trained on pattern recognition) sees a new 'hack' of an application, it probably doesn't have a clear understanding of what's going on, or how to address it. 

In this case, the agent hung trying to decode the encoded file because the original author of the code did some fancy encoding of the string. It's only after you guide the agent was it able to resolve the issue.

It's also the reason why I believe it's impossible to one-shot the solution to this problem.

## 
